{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120740cc-1ba9-44ae-a673-1047f23de335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, Dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b7a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def read_and_tokenize_data(train,test,evaluate):\n",
    "    d = load_dataset('pandas', data_files={\"train\":train, \"test\":test,\"evaluate\":evaluate})\n",
    "    for i in d:\n",
    "        d[i]= d[i].remove_columns(column_names = ['__index_level_0__'])\n",
    "    d = d.map(tokenize_function, batched=True)\n",
    "    return d\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e349d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaPreTrainedModel, RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "class RobertaForSequenceClassification2(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "        self.num_labels = kwargs.get(\"task_labels_map\", {})\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "    \n",
    "        self.classifier1 = nn.Linear(\n",
    "            config.hidden_size, list(self.num_labels.values())[0]\n",
    "        )\n",
    "        self.classifier2 = nn.Linear(\n",
    "            config.hidden_size, list(self.num_labels.values())[1]\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        task_name=None\n",
    "    ):\n",
    "\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        \n",
    "        logits = None\n",
    "        if task_name == list(self.num_labels.keys())[0]:\n",
    "            logits = self.classifier1(pooled_output)\n",
    "        elif task_name == list(self.num_labels.keys())[1]:\n",
    "            logits = self.classifier2(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels[task_name]), label.view(-1)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de8beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "class NLPDataCollator:\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Union[InputDataClass, Dict]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "            # NLP data sets current works presents features as lists of dictionary\n",
    "            # (one per example), so we  will adapt the collate_batch logic for that\n",
    "            if \"label\" in first and first[\"label\"] is not None:\n",
    "                if first[\"label\"].dtype == torch.int64:\n",
    "                    label = torch.tensor(\n",
    "                        [f[\"label\"] for f in features], dtype=torch.long\n",
    "                    )\n",
    "                else:\n",
    "                    label = torch.tensor(\n",
    "                        [f[\"label\"] for f in features], dtype=torch.float\n",
    "                    )\n",
    "                batch = {\"label\": label}\n",
    "            for k, v in first.items():\n",
    "                if k != \"label\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "            # otherwise, revert to using the default collate_batch\n",
    "            return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader)\n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])\n",
    "\n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_sampler = (\n",
    "            RandomSampler(train_dataset)\n",
    "            if self.args.local_rank == -1\n",
    "            else DistributedSampler(train_dataset)\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                sampler=train_sampler,\n",
    "                collate_fn=self.data_collator,\n",
    "            ),\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each\n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader(\n",
    "            {\n",
    "                task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "                for task_name, task_dataset in self.train_dataset.items()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03489c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-571dfb15c0701b97\n",
      "Reusing dataset pandas (/scratch/lustre/home/doma6660/.cache/huggingface/datasets/pandas/default-571dfb15c0701b97/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6a783d5f014ba893a960986a17cf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch/lustre/home/doma6660/.cache/huggingface/datasets/pandas/default-571dfb15c0701b97/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade/cache-5dbbbdef9f032114.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd863cba6201434dbea14ff6a729ba10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f56ad01fcc347d9bfd755e07e6f8b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f45029a52432aaca\n",
      "Reusing dataset pandas (/scratch/lustre/home/doma6660/.cache/huggingface/datasets/pandas/default-f45029a52432aaca/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c687f0a4a8444a118e0602136396ada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92348fd7f49f4e3a8a963cc09b823cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075a77d8d4754af19fb58360842dd217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404a87352f4b4bdb96f8f202f1278563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = {\n",
    "    \"author\":  read_and_tokenize_data(\"data/authors_train.pkl\",\"data/authors_test.pkl\",\"data/authors_validation.pkl\"),\n",
    "    \"sentiment\":  read_and_tokenize_data(\"data/imdb_train.pkl\",\"data/imdb_test.pkl\",\"data/imdb_validation.pkl\")\n",
    "}\n",
    "\n",
    "\n",
    "multitask_model = RobertaForSequenceClassification2.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        task_labels_map={\"author\": 4, \"sentiment\": 2},\n",
    "    )\n",
    "\n",
    "\n",
    "for i in dataset_dict.values():\n",
    "    for j in i.values():\n",
    "        j.set_format(type=\"torch\",columns= [\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    \n",
    "train_dataset = {\n",
    "        task_name: dataset[\"train\"] for task_name, dataset in dataset_dict.items()}\n",
    "\n",
    "eval_dataset = {\n",
    "        task_name: dataset[\"evaluate\"] for task_name, dataset in dataset_dict.items()}\n",
    "\n",
    "test_dataset = {\n",
    "        task_name: dataset[\"evaluate\"] for task_name, dataset in dataset_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548c4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=NLPDataCollator(),\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b498e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5391\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13500\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(trainer,tokenizer,trainer_path,tokenizer_path):\n",
    "    trainer.save_model(trainer_path)\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "save_model(trainer,tokenizer,\"models/multihead_classification/model\",\"models/multihead_classification/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27cf7b-a816-42a0-8a21-f44fdac79aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2cd1d-1ce0-4ae4-9aa9-47e4f06ae760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

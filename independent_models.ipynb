{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120740cc-1ba9-44ae-a673-1047f23de335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, Dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17b7a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at C:\\Users\\dovyd/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at C:\\Users\\dovyd/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\dovyd/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\dovyd/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def read_and_tokenize_data(train,test,evaluate):\n",
    "    d = load_dataset('pandas', data_files={\"train\":train, \"test\":test,\"evaluate\":evaluate})\n",
    "    for i in d:\n",
    "        d[i]= d[i].remove_columns(column_names = ['__index_level_0__'])\n",
    "    d = d.map(tokenize_function, batched=True)\n",
    "    return d\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a796187-c8ee-4a74-ace5-199fcaa15b84",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "154ee52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e38bfbbbe1600378\n",
      "Reusing dataset pandas (C:\\Users\\dovyd\\.cache\\huggingface\\datasets\\pandas\\default-e38bfbbbe1600378\\0.0.0\\6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da9aa031f7549f999cf25ba84e3d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254088ad3a934e4681548cd29bbe0242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccb89ab301e4a209782adcb3168e07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d2b8b7ed254b57bd954e72d478a984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2ce70cf2cb0a1aba\n",
      "Reusing dataset pandas (C:\\Users\\dovyd\\.cache\\huggingface\\datasets\\pandas\\default-2ce70cf2cb0a1aba\\0.0.0\\6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ec5ba5787473cbfab0a619d63ebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de568fb0a794ebaac4112c62ab4b7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681cbc29ff342368a6720fd1ae18429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f2479ba86e47dcb77c644ca47ae391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dict = read_and_tokenize_data(\"data/authors_train.pkl\",\\\n",
    "                               \"data/authors_test.pkl\",\\\n",
    "                                   \"data/authors_validation.pkl\")\n",
    "\n",
    "data_dict_2 = read_and_tokenize_data(\"data/imdb_train.pkl\",\\\n",
    "                               \"data/imdb_test.pkl\",\\\n",
    "                                   \"data/imdb_validation.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932f74d-ce1f-4020-b142-f9ed7e820763",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a545e-10c3-4b82-9a1e-edec0242abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 4)\n",
    "\n",
    "model_2 = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d31fa26e-b3e2-482f-a66e-9f870ca2620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 250,\n",
    "    save_steps = 250,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    train_dataset=data_dict[\"train\"],\n",
    "    eval_dataset=data_dict[\"evaluate\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer_2 = Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    train_dataset=data_dict_2[\"train\"],\n",
    "    eval_dataset=data_dict_2[\"evaluate\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c361e8a-bb8b-4a45-a270-4bd8eede77dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151a0be",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 2700\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6760\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-250\n",
      "Configuration saved in results/checkpoint-250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3027671277523041, 'eval_accuracy': 0.92, 'eval_runtime': 1.4398, 'eval_samples_per_second': 104.18, 'eval_steps_per_second': 13.196, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3491, 'learning_rate': 1.85207100591716e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36780261993408203, 'eval_accuracy': 0.92, 'eval_runtime': 1.435, 'eval_samples_per_second': 104.532, 'eval_steps_per_second': 13.241, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-750\n",
      "Configuration saved in results/checkpoint-750/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4670043885707855, 'eval_accuracy': 0.9133333333333333, 'eval_runtime': 1.4344, 'eval_samples_per_second': 104.576, 'eval_steps_per_second': 13.246, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1568, 'learning_rate': 1.70414201183432e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3702942728996277, 'eval_accuracy': 0.94, 'eval_runtime': 1.431, 'eval_samples_per_second': 104.819, 'eval_steps_per_second': 13.277, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-1250\n",
      "Configuration saved in results/checkpoint-1250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39924490451812744, 'eval_accuracy': 0.9333333333333333, 'eval_runtime': 1.4317, 'eval_samples_per_second': 104.773, 'eval_steps_per_second': 13.271, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-1250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0491, 'learning_rate': 1.5562130177514792e-05, 'epoch': 4.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4040314257144928, 'eval_accuracy': 0.9466666666666667, 'eval_runtime': 1.4311, 'eval_samples_per_second': 104.815, 'eval_steps_per_second': 13.277, 'epoch': 4.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-1750\n",
      "Configuration saved in results/checkpoint-1750/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45243576169013977, 'eval_accuracy': 0.9466666666666667, 'eval_runtime': 1.4336, 'eval_samples_per_second': 104.631, 'eval_steps_per_second': 13.253, 'epoch': 5.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0386, 'learning_rate': 1.4082840236686392e-05, 'epoch': 5.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42107582092285156, 'eval_accuracy': 0.94, 'eval_runtime': 1.4328, 'eval_samples_per_second': 104.691, 'eval_steps_per_second': 13.261, 'epoch': 5.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-2250\n",
      "Configuration saved in results/checkpoint-2250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7464847564697266, 'eval_accuracy': 0.9133333333333333, 'eval_runtime': 1.436, 'eval_samples_per_second': 104.458, 'eval_steps_per_second': 13.231, 'epoch': 6.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-2250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0216, 'learning_rate': 1.2603550295857989e-05, 'epoch': 7.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7705973386764526, 'eval_accuracy': 0.9066666666666666, 'eval_runtime': 1.4346, 'eval_samples_per_second': 104.556, 'eval_steps_per_second': 13.244, 'epoch': 7.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-2750\n",
      "Configuration saved in results/checkpoint-2750/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7515062093734741, 'eval_accuracy': 0.92, 'eval_runtime': 1.4336, 'eval_samples_per_second': 104.635, 'eval_steps_per_second': 13.254, 'epoch': 8.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-2750/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2750/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0147, 'learning_rate': 1.1124260355029586e-05, 'epoch': 8.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8225696682929993, 'eval_accuracy': 0.9066666666666666, 'eval_runtime': 1.4348, 'eval_samples_per_second': 104.546, 'eval_steps_per_second': 13.242, 'epoch': 8.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-3250\n",
      "Configuration saved in results/checkpoint-3250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7690063714981079, 'eval_accuracy': 0.92, 'eval_runtime': 1.4569, 'eval_samples_per_second': 102.958, 'eval_steps_per_second': 13.041, 'epoch': 9.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-3250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.012, 'learning_rate': 9.644970414201184e-06, 'epoch': 10.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7602660059928894, 'eval_accuracy': 0.9133333333333333, 'eval_runtime': 1.4358, 'eval_samples_per_second': 104.468, 'eval_steps_per_second': 13.233, 'epoch': 10.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-3750\n",
      "Configuration saved in results/checkpoint-3750/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7954994440078735, 'eval_accuracy': 0.92, 'eval_runtime': 1.438, 'eval_samples_per_second': 104.311, 'eval_steps_per_second': 13.213, 'epoch': 11.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0055, 'learning_rate': 8.165680473372781e-06, 'epoch': 11.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7632479071617126, 'eval_accuracy': 0.92, 'eval_runtime': 1.4372, 'eval_samples_per_second': 104.372, 'eval_steps_per_second': 13.221, 'epoch': 11.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-4250\n",
      "Configuration saved in results/checkpoint-4250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6230855584144592, 'eval_accuracy': 0.9333333333333333, 'eval_runtime': 1.4365, 'eval_samples_per_second': 104.418, 'eval_steps_per_second': 13.226, 'epoch': 12.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-4250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0071, 'learning_rate': 6.686390532544379e-06, 'epoch': 13.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6589962840080261, 'eval_accuracy': 0.9333333333333333, 'eval_runtime': 1.4411, 'eval_samples_per_second': 104.088, 'eval_steps_per_second': 13.184, 'epoch': 13.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-4750\n",
      "Configuration saved in results/checkpoint-4750/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7136066555976868, 'eval_accuracy': 0.9266666666666666, 'eval_runtime': 1.4372, 'eval_samples_per_second': 104.369, 'eval_steps_per_second': 13.22, 'epoch': 14.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-4750/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 5.207100591715976e-06, 'epoch': 14.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7986064553260803, 'eval_accuracy': 0.92, 'eval_runtime': 1.4356, 'eval_samples_per_second': 104.486, 'eval_steps_per_second': 13.235, 'epoch': 14.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-5250\n",
      "Configuration saved in results/checkpoint-5250/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9433311223983765, 'eval_accuracy': 0.9133333333333333, 'eval_runtime': 1.4386, 'eval_samples_per_second': 104.268, 'eval_steps_per_second': 13.207, 'epoch': 15.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5250/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 3.7278106508875745e-06, 'epoch': 16.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7859207987785339, 'eval_accuracy': 0.92, 'eval_runtime': 1.4347, 'eval_samples_per_second': 104.549, 'eval_steps_per_second': 13.243, 'epoch': 16.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer_2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fccf4-232a-4537-89c2-942ac12c74d2",
   "metadata": {},
   "source": [
    "#### Prediction and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddfe04f2-d949-4338-8d65-69fc1d6bab64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions(trainer,test):\n",
    "    predictions = trainer.predict(test)\n",
    "    y_true = test[\"label\"]\n",
    "    y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "    confusion_matrix(y_true, y_pred)\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    metric.compute(predictions=y_pred, references=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a845ec",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73908b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(trainer,tokenizer,trainer_path,tokenizer_path):\n",
    "    trainer.save_model(trainer_path)\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "save_model(trainer,tokenizer,\"models/author_classification/model\",\"models/author_classification/tokenizer\")\n",
    "save_model(trainer_2,tokenizer,\"models/sentiment_classification/model\",\"models/sentiment_classification/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9710a8",
   "metadata": {},
   "source": [
    "#### Loading this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7953079",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models_2/author_classification/model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models_2/author_classification/model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file models_2/author_classification/model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at models_2/author_classification/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "Didn't find file models_2/author_classification/tokenizer\\added_tokens.json. We won't load it.\n",
      "Didn't find file models_2/author_classification/tokenizer\\tokenizer.json. We won't load it.\n",
      "loading file models_2/author_classification/tokenizer\\vocab.json\n",
      "loading file models_2/author_classification/tokenizer\\merges.txt\n",
      "loading file None\n",
      "loading file models_2/author_classification/tokenizer\\special_tokens_map.json\n",
      "loading file models_2/author_classification/tokenizer\\tokenizer_config.json\n",
      "loading file None\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file models_2/sentiment_classification/model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"models_2/sentiment_classification/model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file models_2/sentiment_classification/model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at models_2/sentiment_classification/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "Didn't find file models_2/sentiment_classification/tokenizer\\added_tokens.json. We won't load it.\n",
      "Didn't find file models_2/sentiment_classification/tokenizer\\tokenizer.json. We won't load it.\n",
      "loading file models_2/sentiment_classification/tokenizer\\vocab.json\n",
      "loading file models_2/sentiment_classification/tokenizer\\merges.txt\n",
      "loading file None\n",
      "loading file models_2/sentiment_classification/tokenizer\\special_tokens_map.json\n",
      "loading file models_2/sentiment_classification/tokenizer\\tokenizer_config.json\n",
      "loading file None\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path,tokenizer_path):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 4)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "    trainer = Trainer(model = model)\n",
    "    return (model, trainer, tokenizer)\n",
    "    \n",
    "\n",
    "model, trainer, tokenizer = load_model(\"models/author_classification/model\",\"models/author_classification/tokenizer\")\n",
    "\n",
    "model_2, trainer_2, _ = load_model(\"models/sentiment_classification/model\",\"models/sentiment_classification/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0c120f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-05868a22cc33adaa\n",
      "Reusing dataset pandas (C:\\Users\\dovyd\\.cache\\huggingface\\datasets\\pandas\\default-05868a22cc33adaa\\0.0.0\\6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ff468baab747f79bdb441385f997eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 90\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 01:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  0  0]\n",
      " [ 0  9  0  0]\n",
      " [ 0  0  4  0]\n",
      " [ 1  1  0 49]]\n",
      "{'accuracy': 0.9777777777777777}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "def read_and_tokenize_data(data):\n",
    "    d = load_dataset('pandas', data_files=data,split=\"train\")\n",
    "    d = d.map(tokenize_function, batched=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def make_predictions(trainer,data_path):\n",
    "    test_data = read_and_tokenize_data(data_path)\n",
    "    predictions = trainer.predict(test_data)\n",
    "    y_true = test_data[\"label\"]\n",
    "    y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "    print(confusion_matrix(test_data[\"label\"], y_pred))\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    print(metric.compute(predictions=y_pred, references=y_true))\n",
    "\n",
    "\n",
    "\n",
    "make_predictions(trainer,\"data/authors_small_train.pkl\")\n",
    "\n",
    "make_predictions(trainer_2,\"data/imdb_small_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f7489cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14df73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.7999024391174316}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = TextClassificationPipeline(model=model,tokenizer=tokenizer)\n",
    "\n",
    "pipeline(\"Classify this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbaf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_2 = TextClassificationPipeline(model=model_2,tokenizer=tokenizer)\n",
    "\n",
    "pipeline_2(\"Classify this!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
